This chapter discusses inter-industry studies of the relations among various measures of market structure, conduct, and performance. It discusses that tradition has indeed uncovered many stable, robust, and empirical regularities. Inter-industry research has taught much about the way markets look, especially within the manufacturing sector in developed economies, even if it has not shown exactly the way markets work. Work in some areas has produced no clear picture of the important patterns in the data, and non-manufacturing industries have not received attention commensurate with their importance. But cross-section studies are limited by serious problems of interpretation and measurement. Future inter-industry research should adopt a modest, descriptive orientation and aim to complement case studies by uncovering robust empirical regularities that can be used to evaluate and develop theoretical tools. Much of the most persuasive recent work relies on nonstandard data sources, particularly panel data that can be used to deal with disequilibrium problems and industry-specific data, which mitigate the problem of unobservable industry-specific variables.	1980-1989
This chapter describes the econometric studies of market power in single markets and in groups of related markets. The recent increase in the number of such studies and substantial advances in the methods for carrying them out constitute a dramatic shift in the focus of empirical work in the industrial organization (IO) field. The chapter discusses time series data from single industries or on data from closely related markets. The new empirical industrial organization (NEIO) is clearly somewhat different than the previously dominant empirical method in the field, the structure–conduct–performance paradigm (SCPP). The chapter presents the formation and enforcement of tacitly collusive arrangements, the nature of noncooperative oligopoly interaction in the world, the degree of single-firm market power under product differentiation, and the size and determinants of the industry price-cost margin. It reviews the various empirical models of monopoly power and of oligopoly interaction and describes the theoretical and empirical arguments for why it is monopoly power that is being measured.	1980-1989
The effect on social welfare of third-degree price discrimination was first investigated by Joan Robinson (1933). Richard Schmalensee (1981) has recently reexamined this question and presented several new results. In particular, he noted that a necessary condition for  price discrimination to increase social wel-  fare-defined as consumers' plus producers'  surplus- is that output increase.  Schmalensee established this result only in  the case of independent demands and con-  stant marginal costs. However, it turns out to  be true in much more general circumstances.  In this paper I show how simple methods  from duality theory can be used to establish  this result and several other new results on  the welfare effect of price discrimination.	1980-1989
The purpose of this paper is to analyze the working of the used car market, a market suffering severe informational asymmetry, by generalizing in two respects works of George Akerlof (1970) and Charles Wilson (1980) in the Walrasian paradigm.' First, although Akerlof correctly points out a possible market failure in the used car market, his description of it overlooks the ability of each agent to freely choose whether to be a buyer or a seller. In other words, in the used car market, unlike other markets with informational asymmetry- the insurance market and the labor market-an agent can change his position from buyer to seller, or vice versa, with little or no transaction cost. Instead, Akerlof (and Wilson) arbitrarily divides the agents in the market into two groups, buyers and sellers, where, for example, an agent in the buyer group is supposed to buy only a used car. This is unnecessarily restrictive. It is possible that, if the price of used cars goes up, a used car buyer may want to shift his demand from a used car to a new car, and as a result he will be a used car seller. Second, my model treats the quality of used cars as an endogenous variable in contrast with the Akerlof-Wilson model where each seller is exogenously endowed with a car of given quality. However, it is well recognized that the quality of a car depends not only on purely stochastic elements as perceived by Akerlof and Wilson, but also on its owner through endogenous factors such as maintenance, driving habits, and the like. This paper places more emphasis on the latter by assuming that the quality of a car is a function of the maintenance level. Throughout the analysis, however, I will preserve the informational structure of Akerlof and Wilson by assuming that no activities of information acquisition and transmission (i.e., signaling, warranty, or search) are allowed. As a result, potential buyers make their decision based only on the average quality of used cars.	1980-1989
Proposes to develop a service quality model, based on test of a sample of business executives, which describes how the quality of services is perceived by customers. Looks at its marketing implications, in which functional quality is seen to be a very important dimension of a perceived service. Concludes that quality dimensions are interrelated and that the importance of image should be recognised.	1980-1989
"Cicero demands of historians, first, that we tell true stories. I intend fully to perform my duty on this occasion, by giving you a homely piece of narrative economic history in which ""one damn thing follows another."" The main point of the story will become plain enough: it is sometimes not possible to uncover the logic (or illogic) of the world around us except by understanding how it got that way. A path-dependent sequence of economic changes is one of which important influences upon the eventual outcome can be exerted by temporally remote events, including happen- ings dominated by chance elements rather than systematic forces. Stochastic processes like that do not converge automatically to a fixed-point distribution of outcomes, and are called non-ergodic. In such circumstances """"historical accidents"" can neither be ignored, nor neatly quarantined for the purpose of economic analysis; the dynamic process itself takes on an essentially historical character. Standing alone, my story will be simply il- lustrative and does not establish how much of the world works this way. That is an open empirical issue and I would be presumptuous to claim to have settled it, or to instruct you in what to do about it. Let us just hope the tale proves mildly diverting for those wait- ing to be told if and why the study of eco- nomic history is a necessity in the making of economists."	1980-1989
There are many products for which the utility that a user derives from consumption of the good increases with the number of other agents consuming the good. There are several possible sources of these positive consumption externalities.1 1) The consumption externalities may be generated through a direct physical effect of the number of purchasers on the quality of the product. The utility that a consumer derives from purchasing a telephone, for ex- ample, clearly depends on the number of other households or businesses that have joined the telephone network. These network externalities are present for other communications technologies as well, including Telex, data networks, and over-the-phone facsimilie equipment. 2) There may be indirect effects that give rise to consumption externalities. For example, an agent purchasing a personal computer will be concerned with the number of other agents purchasing similar hardware because the amount and variety of software that will be supplied for use with a given computer will be an increasing function of the number of hardware units that have been sold. This hardware-software paradigm also applies to video games, video players and recorders, and phonograph equipment. 3) Positive consumption externalities arise for a durable good when the quality and availability of postpurchase service for the good depend on the experience and size of the service network, which may in turn vary with the number of units of the good that have been sold. In the automobile market, for example, foreign manufacturers' sales initially were retarded by consumers' awareness of the less experienced and thinner service networks that existed for new or less popular brands.	1980-1989
"The phenomenon of stars is defined by Sherwin Rosen to be one ""wherein relatively small numbers of people earn enormous amounts of money and dominate the activities in which they engage"" (1981, p. 845). Rosen sets out to explain two aspects of this phenomenon: persons with only a slightly greater talent command much higher incomes than those who are only slightly less talent- ed; output is concentrated on those few who have the most talent. Rosen's explanation consists of two factors: lesser talent is a poor substitute for greater talent, and either the activity can be reproduced endlessly (for example, on records) at a fixed cost, or the cost of production does not rise in proportion to the size of the seller's market (a better surgeon can perform better operations and more of them within a given time). Rosen explains why large differences in earnings could exist where there are only small differences in talent. This paper ex- plains why large differences in earnings could exist even where there are no differences in talent at all. In other words, it explains why there could be stars among individuals known to have equal talents."	1980-1989
Stokey [1979] considers the case of a monopolist selling in continuous time to heterogeneous consumers with unit demands. The seller can precommit to any price path. By charging declining prices, the monopolist can induce customers with higher valuations to purchase sooner than those with lower valuations. Although price discrimination by inducing self-selection is, therefore, feasible in her model, Stokey shows that (under her base-case assumptions) it is never advantageous. It is always optimal for the monopolist to precommit to a fixed price over time-thereby inducing everyone who would earn surplus at that price to purchase at the first opportunity. This is a striking result-and, in light of the related papers of Spence [1977, 1980] and Mussa and Rosen [1978] -a puzzling one. Spence showed that often the unique optimal strategy for a monopolist is to induce self-selection among heterogeneous customers by offering a menu of quantities, each requiring a different outlay. Mussa-Rosen showed in the quality context that nonlinear pricing to induce self-selection is likewise often strictly superior to any other strategy. How can second-degree price discrimination often be optimal in these latter contexts but never advantageous in the intertemporal one? To facilitate a comparison of these models, we extract their essential features and treat them in a common framework. Stokey's result that price discrimination, although feasible, is never advantageous is shown to have its counterpart in these other models if analogous curvature assumptions are made. While these are not the assumptions made by Spence or Mussa-Rosen, Stokey's result nonetheless has important and unnoticed implications for the contexts these authors considered.	1980-1989
"In both market and planned economies, organizations rely on communication of information about realized levels of performance or states of nature. For instance, lenders and insurers rely on clients' reports of income; central planning bureaus rely on performance reports from productive enterprises under their direction; and revenue collection agencies rely on income reports from tax- payers. In all these examples, the reporting agents have an incentive to misrepresent because transfers between the parties depend on their reports. In practice, an important tool of the recipient of reports (the principal) is the right to audit the information communicated by the agent. Such audits usually entail significant costs. Consequently, the principal would generally prefer to audit the agents' reports selectively. This gives rise to the question: what is the optimal nature of audit strategies, and how are these related to the structure of optimal transfers?"	1980-1989
Through what selling strategy can a multiproduct monopolist maximize his profits when his knowledge about individual consumers' preferences is limited? One possibility, extensively studied in the context of a single-good monopoly, is to use quantity-dependent pricing as a means of discriminating among customers with differing tastes (see, for example, Oi [1971] and Maskin and Riley [1984]). An alternative technique for price discrimination, first suggested by Stigler [1968] and analyzed further by Adams and Yellen [1976], is for the monopolist to package two or more products in bundles rather than selling them separately.' Through a series of examples Adams and Yellen illustrate that bundling can serve as a useful price discrimination device, even when all consumers' willingnesses to pay for each of the goods individually are unaffected by whether they are also consuming the other product. A typical example is illustrated in Figure I (adapted from Figure IV in Adams and Yellen), where there are two goods, three consumers (AB,C) who consume at most one unit of each good (with reservation values for each good that are independent of whether the other good is consumed), and zero costs of production. There, a bundle offered at a price of 100 fully extracts all potential surplus, which would be impossible pricing the goods independently. Unfortunately, though, these authors do not provide any general characterization of the circumstances in which bundling is actually a multiproduct monopolist's optimal strate	1980-1989
"In a recent paper, Rubinstein and Wolinsky (1985) follow Diamond (1981, 1982), Mortensen (1982a, b) and others' in considering a market with two types of agent. Type 1 are sellers and type 2 are buyers of an indivisible good (e.g. a house). On entering the market, agents wait to be matched with an agent of the opposite type. Once a pair is matched, they bargain. The bargaining continues until agreement is reached or else one or both of the agents is matched elsewhere. After reaching agreement, both agents immediately leave the market. Rubinstein and Wolinsky assume an exogenous equilibriating mechanism which ensures that exits from the market are matched precisely by new entries, thus maintaining the numbers of the two types at a steady level. With this and other ""stationary"" assumptions, they demonstrate the existence of a unique equilibrium. Binmore and Herrero (1984, 1987) show that equilibria in such markets are unique, without any stationary assumptions at all, provided that the agents are equipped with discount factors satisfying 0? 8< 1 (i = 1, 2). The current paper characterizes the unique equilibrium, for the case of continuous time and infinite numbers of agents of both types, in terms of the prob- abilities (iF(t) (i = 1, 2) that an agent of type i will not find a bargaining partner between time 0 and t. In principle, these probabilities, 'F1(t) and ?2(t), can be calculated from a knowledge of the search technology and the rates at which agents enter and leave the market. We carry through this enterprise for the simplest of search technologies in the case when agents of both types flow into the market at an equal exogenously determined rate.2 (The rate at which agents flow out of the market is determined by the equilibrium analysis.)"	1980-1989
The elimination of technical barriers to trade in the EC is one of the most important routes to achieve a unified, genuinely free Internal Market in the Community.  In  the  one  and  a  half  decades  since  the  comprehensive Commission  proposals  for  technical  harmonization  were  first  published, great difficulties have  been  encountered and progress has been  slow. An alteration of this ‘traditional’ approach and getting Member States to accept other methods was long overdue in the early 1980s. The ‘new approach’ to harmonization and standardization, initiated  in 1985, is  an  attempt  to  accelerate  both  harmonization  processes  at  the Council  level  and  European standardization  processes  at  industry  level while at the same time providing more flexibility for innovation and easier market access. It is potentially of great economic importance.’ This paper examines  the constituent  elements of the  new  approach, evaluates it by means of a systematic comparison with the drawbacks of the old approach and identifies problems that should be dealt with promptly.	1980-1989
"This research derives its motivation (and borrows unrestrainedly) from socio- logical studies of collusive behavior in organizations. Like the sociology literature, it emphasizes that behavior is often best predicted by the analysis of group as well as individual incentives; and it gropes toward a precise definition of concepts such as ""power,"" ""cliques,"" ""corporate politics,"" and ""bureaucracy"" (Crozier, 1963; Cyert and March; Dalton; Scott). It differs from this literature in that it tries to incorporate the acquired knowledge of modern information economics into the analysis. The research also borrows a considerable amount from the principal/agent paradigm of information economics. This paradigm, mainly developed for two-tier organizations, emphasizes the productive inefficiency associated with asymmetric information and insurance motives (or limited liability cons- traints).' Formally, organizations can be seen as networks of overlapping or nested principal/agent relationships. A theme of the paper, however, is that the analysis of hierarchical structures does not boil down to a compounding of the basic inefficiency, due to the fact that going from the simple two-tier  principal/agent structure to more complex ones introduct collusion. This research departs from the existing information economics literature in that it views an organization as a network of contracts that interplay rather than as a single contract."	1980-1989
"In this paper and its sequel (Dasgupta and Maskin (1986)) we study the existence of Nash equilibrium in games where agents' payoff (or utility) functions are discontinuous. Our enquiry is motivated by a number of recent studies that have uncovered serious existence problems in seemingly innocuous economic games. These examples include models of spatial competition (Eaton and Lipsey (1975) and Shaked (1975)), Hotelling's model of price competition (d'Aspremont, Gabszewicz and Thisse (1979)), and models of market-dependent information (e.g. the insurance-market example of Rothschild and Stiglitz (1976) and Wilson (1977)). In fact, the non-existence of Nash equilibrium in simple economic models was noted long ago by Edgeworth (1925) in his critique of Bertrand's (1883) analysis of price setting duopolists; (see Chamberlin (1956), pp. 34-46). These examples can be readily cast as games in normal form with continuous strategy sets (see the sequel to this paper). Since they fail in general to possess an equilibrium they must obviously violate one or more of the hypotheses of the classical existence theorem for games of this type (e.g. Debreu (1952), Glicksberg (1952), and Fan (1952)). These hypotheses typically include continuity and a limited form of quasi-concavity of the payoff functions (in addition to the usual convexity and compactness assumptions on strategy sets). To better understand a model that does not possess an equilibrium it is helpful to identify the failure-violation of quasi-concavity or of continuity-to which the non-existence can be attributed. Drawing such a distinction allows us to construct a taxonomy of ""equilibrium failure"" and to highlight the structural similarities or differences among models. It also enables us to see how the models may be plausibly modified to restore the existence of equilibrium. This is important because the failure of an economic model to possess an equilibrium has sometimes led economists to cast doubt on the validity-and even the internal consistency-of the construct"	1980-1989
This paper considers a class of principal-agent problems which have the following features. (1) There is adverse selection because the principal ignores the value of one parameter of the agent’s true characteristics. (2) Leaving aside the information parameter, the principal’s welfare as well as the agent’s welfare depend on two types of variables, observable to both of them. The first ones, possibly multidimensional, are called action variable(s), and the second one, which is one-dimensional, has in general the meaning of a money transfer. (3) The principal is a Stackelberg leader of the two-person game. He can commit himself to decision rules which are admissible on informational grounds. He optimizes within the adequate class, taking into account, besides the agent’s reaction, one constraint which has generally the meaning of an individual rationality constraint and sometimes of a feasibility constraint. The optimization is limited to the class of non-stochastic mechanisms. (4) The problem can also receive an alternative interpretation: the principal faces a continuum of agents of unknown characteristics (the distribution being known, however).	1980-1989
ADAM SMITH postulated three types of public goods: safety, justice and public works which it would not pay private individuals to produce, all three summed up in the word ‘magistracy’, which was the necessary pre- requisite for free trade [1776, pp. 653, 669, 681/2]. Magistracy can be ex- tended from this group or made to include within it such elusive ‘goods’ as macro-economic stability, redistribution of national income, the monetary system, and of interest here, standards of various sorts. Standards of measurement - whether linear, weight, bulk, temperature, time  or value (i. e. the  unit-of-account  function of  money)  clearly fall  within SAMUELSON’S definition of public goods in that they are available for use by all and that use by any one economic actor does not reduce the amount available to others [1954]. In fact they are a strong form of public good in that they have economies of scale. The more producers and consumers use a given standard, the more each gains from use by  others through gains in comparability and interchangeability. My concern goes beyond standards of weights, measures, temperature, time  and value, though  they  play important roles in what  follows, to include standards determined by various groups - governments, trade or professional associations, and even companies - limiting the characteristics of goods or products, and to deal with the setting, diffusion, and changing of  standards.	1980-1989
